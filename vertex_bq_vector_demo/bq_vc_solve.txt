
Here's a comprehensive workflow for embedding mixed-type data and using it with BigQuery or Vertex AI Vector Search:

Overall Workflow
1. Data Preprocessing & Feature Engineering
First, you need to convert mixed-type data into a format suitable for embedding:

python
import pandas as pd
from google.cloud import bigquery

# Load your data
client = bigquery.Client()
query = """
SELECT 
    id,
    string_col,
    int_col,
    float_col,
    text_col
FROM `your-project.your_dataset.your_table`
"""
df = client.query(query).to_dataframe()

# Combine features into text representation
def create_text_representation(row):
    parts = []
    parts.append(f"Description: {row['text_col']}")
    parts.append(f"Category: {row['string_col']}")
    parts.append(f"Value: {row['int_col']}")
    parts.append(f"Score: {row['float_col']}")
    return " | ".join(parts)

df['combined_text'] = df.apply(create_text_representation, axis=1)
2. Generate Embeddings
Use Vertex AI's text embedding model:

python
from vertexai.language_models import TextEmbeddingModel

model = TextEmbeddingModel.from_pretrained("text-embedding-004")

def get_embeddings(texts, batch_size=250):
    embeddings = []
    for i in range(0, len(texts), batch_size):
        batch = texts[i:i+batch_size]
        batch_embeddings = model.get_embeddings(batch)
        embeddings.extend([emb.values for emb in batch_embeddings])
    return embeddings

embeddings = get_embeddings(df['combined_text'].tolist())
df['embedding'] = embeddings
3. Choose Storage Option
Option A: BigQuery Vector Search (Recommended for SQL-based workflows)
Advantages:

Native SQL querying
No additional infrastructure
Integrated with existing BigQuery tables
Cost-effective for moderate scale
python
# Create table with vector column
create_table_query = """
CREATE OR REPLACE TABLE `your-project.your_dataset.embeddings_table` (
    id STRING,
    string_col STRING,
    int_col INT64,
    float_col FLOAT64,
    text_col STRING,
    combined_text STRING,
    embedding ARRAY<FLOAT64>
)
"""
client.query(create_table_query).result()

# Load embeddings
df['embedding'] = df['embedding'].apply(lambda x: list(x))
job = client.load_table_from_dataframe(
    df, 
    'your-project.your_dataset.embeddings_table'
)
job.result()

# Create vector index for faster search
create_index_query = """
CREATE VECTOR INDEX my_index
ON `your-project.your_dataset.embeddings_table`(embedding)
OPTIONS(distance_type='COSINE', index_type='IVF')
"""
client.query(create_index_query).result()
Perform similarity search:

python
# Generate query embedding
query_text = "your search query"
query_embedding = model.get_embeddings([query_text])[0].values

# Search using SQL
search_query = f"""
SELECT 
    base.id,
    base.text_col,
    base.string_col,
    distance
FROM 
    VECTOR_SEARCH(
        TABLE `your-project.your_dataset.embeddings_table`,
        'embedding',
        (SELECT {list(query_embedding)} as embedding),
        distance_type => 'COSINE',
        top_k => 10
    )
"""
results = client.query(search_query).to_dataframe()
Option B: Vertex AI Vector Search (Recommended for high-scale, low-latency)
Advantages:

Sub-100ms latency at scale
Handles billions of vectors
Better for real-time applications
Advanced filtering capabilities
python
from google.cloud import aiplatform
from google.cloud.aiplatform.matching_engine import MatchingEngineIndex, MatchingEngineIndexEndpoint

aiplatform.init(project='your-project', location='us-central1')

# 1. Export embeddings to GCS in JSON Lines format
import json
from google.cloud import storage

output_file = 'embeddings_export.jsonl'
with open(output_file, 'w') as f:
    for idx, row in df.iterrows():
        record = {
            'id': str(row['id']),
            'embedding': row['embedding'],
            # Store metadata for filtering
            'restricts': [
                {'namespace': 'string_col', 'allow': [row['string_col']]},
                {'namespace': 'int_range', 'allow': [str(row['int_col'] // 10)]}
            ]
        }
        f.write(json.dumps(record) + '\n')

# Upload to GCS
storage_client = storage.Client()
bucket = storage_client.bucket('your-bucket')
blob = bucket.blob('embeddings/embeddings_export.jsonl')
blob.upload_from_filename(output_file)

# 2. Create Vector Search Index
index = aiplatform.MatchingEngineIndex.create_tree_ah_index(
    display_name='my-vector-index',
    contents_delta_uri='gs://your-bucket/embeddings/',
    dimensions=768,  # text-embedding-004 dimension
    approximate_neighbors_count=150,
    distance_measure_type='COSINE_DISTANCE',
    shard_size='SHARD_SIZE_SMALL'
)

# 3. Deploy index to endpoint
endpoint = aiplatform.MatchingEngineIndexEndpoint.create(
    display_name='my-endpoint',
    public_endpoint_enabled=True
)

deployed_index = endpoint.deploy_index(
    index=index,
    deployed_index_id='deployed_index_id',
    min_replica_count=1,
    max_replica_count=2
)

# 4. Query the endpoint
query_embedding = model.get_embeddings([query_text])[0].values

response = endpoint.match(
    deployed_index_id='deployed_index_id',
    queries=[query_embedding],
    num_neighbors=10
)
Best Practices
For mixed-type data:

Normalize numerical features before concatenating to text
Use semantic field names in your text representation
Consider separate embeddings for very different data types and combine with weighted search
Store original data alongside embeddings for retrieval
Performance optimization:

Use BigQuery for: <10M vectors, batch processing, SQL integration
Use Vertex Vector Search for: >10M vectors, real-time queries (<100ms), production applications
Consider hybrid approach: Store embeddings in BigQuery, export to Vector Search for serving
Cost considerations:

BigQuery: Pay for storage + query processing
Vertex Vector Search: Pay for index nodes (always running)
Would you like me to help you implement a specific part of this workflow or optimize it for your particular use case?


